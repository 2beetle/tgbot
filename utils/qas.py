import asyncio
import datetime
import json
import logging
import pprint
import re
from typing import Tuple
from urllib.parse import urlparse, parse_qs

import pytz
import requests

from config.config import TIME_ZONE, AI_API_KEYS, AI_MODEL, AI_API_KEY, AI_HOST
from utils.ai import openapi_chat

logger = logging.getLogger(__name__)


class QuarkAutoDownload:
    def __init__(self, api_token):
        self.api_token = api_token

    async def data(self, host):
        resp = requests.get(
            f'{host}/data?token={self.api_token}'
        )
        if not resp.ok:
            logger.error(f'Failed to get data {host}, error: {resp.reason}')
        else:
            return resp.json().get('data')

    async def add_job(self, host, task_name, share_url, save_path, pattern, replace):
        resp = requests.post(
            f'{host}/api/add_task?token={self.api_token}',
            headers={
                'Content-Type': 'application/json',
            },
            json={
                'taskname': task_name,
                'shareurl': share_url,
                'savepath': save_path,
                'pattern': pattern,
                'replace': replace
            }
        )
        if not resp.ok:
            logger.error(f'Failed to add task {task_name}, error: {resp.text}')
        return resp

    async def update(self, host, data):
        resp = requests.post(
            f'{host}/update?token={self.api_token}',
            headers={
                'Content-Type': 'application/json',
            },
            json=data
        )
        if not resp.ok:
            logger.error(f'Failed to update data {host}, error: {resp.text}')
        else:
            logger.info(f'Success to update data {host}, data: {data}')
            return resp.json()

    async def get_share_detail(self, host, data):
        resp = requests.post(
            f'{host}/get_share_detail?token={self.api_token}',
            headers={
                'Content-Type': 'application/json',
            },
            json=data
        )
        if not resp.ok:
            logger.error(f'Failed to get_share_detail {host}, error: {resp.text}')
        else:
            logger.info(f'Success to get_share_detail {host}, data: {data}')
            return resp.json()

    async def extract_quark_share_info(self, url: str):
        match = re.search(r'https://pan\.quark\.cn/s/([a-zA-Z0-9]+)', url)
        quark_id = match.group(1) if match else None

        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        pwd = query_params.get('pwd', [""])[0]

        return quark_id, pwd

    async def get_quark_id_stoken_pdir_fid(self, url):
        quark_id, pass_code = await self.extract_quark_share_info(url)
        match = re.search(r'/([^/]+)-[^/]*$', url)
        if match:
            pdir_fid = match.group(1)
        else:
            pdir_fid = None

        if pdir_fid is None:
            if url.endswith('/'):
                pdir_fid = url.split('/')[-2]
            else:
                pdir_fid = url.split('/')[-1]

        if pdir_fid == 'share' or pdir_fid == quark_id:
            pdir_fid = 0

        stoken_resp = requests.post("https://drive-h.quark.cn/1/clouddrive/share/sharepage/token?pr=ucpro&fr=pc",
                             headers={
                                 'Content-Type': 'application/json',
                             },
                             json={
                                 "pwd_id": quark_id,
                                 "passcode": pass_code,
                                 "support_visit_limit_private_share": True
                             })
        if not stoken_resp.ok:
            logger.error(f'Failed to get quark stoken {url}, error: {stoken_resp.text}')
            return quark_id, None, pdir_fid
        stoken = stoken_resp.json()['data']['stoken']
        return quark_id, stoken, pdir_fid

    async def get_quark_dir_detail(self, quark_id, stoken, pdir_fid, include_dir=True):
        sub_resp = requests.get(f'https://drive-h.quark.cn/1/clouddrive/share/sharepage/detail',
                                params={
                                    'pr': 'ucpro',
                                    'fr': 'pc',
                                    'uc_param_str': '',
                                    '_size': 40,
                                    'pdir_fid': pdir_fid,
                                    'pwd_id': quark_id,
                                    'stoken': stoken,
                                    'ver': 2
                                })
        if not sub_resp.ok:
            logger.error(f'Failed to get quark sub {quark_id}/{pdir_fid}, error: {sub_resp.json()}')
            return []
        if include_dir:
            return sub_resp.json()['data']['list']
        else:
            return [file for file in sub_resp.json()['data']['list'] if not file.get('dir')]

    async def get_quark_dir_structure(self, quark_id, stoken, pdir_fid):
        result = list()
        file_list = await self.get_quark_dir_detail(quark_id=quark_id, stoken=stoken, pdir_fid=pdir_fid)
        for file in file_list:
            if file['dir'] is True:
                include_items = await self.get_quark_dir_structure(quark_id=quark_id, stoken=stoken, pdir_fid=file['fid'])
            else:
                include_items = None
            result.append({
                "dir": file['dir'],
                "file_name": file['file_name'],
                "fid": file['fid'],
                "include_items_count": file.get('include_items_count', None),
                "include_items": include_items,
                "last_update_at": datetime.datetime.fromtimestamp(int(file['last_update_at'])/1000,
                                                                  tz=datetime.UTC
                                                                  ).astimezone(pytz.timezone(TIME_ZONE)),
            })
        return result

    async def get_fid_files(self, url: str, include_dir: bool = False):
        async def recursive_get_fid_files(fid, file_name, quark_id, stoken, fid_files, include_dir):
            dir_files = list()
            files = await self.get_quark_dir_detail(quark_id=quark_id, stoken=stoken, pdir_fid=fid)
            for file in files:
                if file['dir'] is True:
                    await recursive_get_fid_files(file['fid'], file['file_name'], quark_id, stoken, fid_files, include_dir)
                    if include_dir:
                        dir_files.append({
                            "file_name": file['file_name'],
                            "dir": file['dir'],
                            "last_update_at": datetime.datetime.fromtimestamp(int(file['last_update_at']) / 1000,
                                                                              tz=datetime.UTC
                                                                              ).astimezone(pytz.timezone(TIME_ZONE)),
                        })
                else:
                    dir_files.append({
                        "file_name": file['file_name'],
                        "dir": file['dir'],
                        "last_update_at": datetime.datetime.fromtimestamp(int(file['last_update_at']) / 1000,
                                                                          tz=datetime.UTC
                                                                          ).astimezone(pytz.timezone(TIME_ZONE)),
                    })
            fid_files[f"{file_name}__{fid}"] = dir_files
            return dir_files

        logger.info(f"Getting fid files for {url}")
        quark_id, stoken, pdir_fid = await self.get_quark_id_stoken_pdir_fid(url)
        if stoken is None:
            return None
        fid_files = dict()
        await recursive_get_fid_files(0, "root", quark_id, stoken, fid_files, include_dir)
        return fid_files

    async def build_unicode_tree_paragraph(self, folder_name: str, files: list) -> str:
        lines = [f"{folder_name}"]
        for i, file in enumerate(sorted(files, key=lambda x: x['file_name'])):
            is_last = i == len(files) - 1
            prefix = 'â””â”€â”€' if is_last else 'â”œâ”€â”€'
            icon = 'ğŸ“‚' if file['dir'] is True else 'ğŸ¥'
            lines.append(f"{prefix} {icon} {file['file_name']}")
        return '\n'.join(lines)

    async def get_tree_paragraphs(self, fid_files: dict) -> list[str]:
        result = []
        for key, files in fid_files.items():
            if key == 'root__0':
                continue
            paragraph = await self.build_unicode_tree_paragraph(key, files)
            result.append(paragraph)
        return result

    async def ai_generate_replace(self, url: str, session, user_id, prompt) -> dict:
        quark_id, stoken, pdir_fid = await self.get_quark_id_stoken_pdir_fid(url=url)
        dir_details = await self.get_quark_dir_detail(quark_id, stoken, pdir_fid, include_dir=False)
        files = [
            {
               "file_name": dir_detail['file_name'],
               "video_max_resolution": dir_detail['video_max_resolution'],
            }
            for dir_detail in dir_details
        ]
        # ä¼˜åŒ–æç¤ºä¿¡æ¯ï¼Œä¸“æ³¨äºReplaceç”Ÿæˆ
        enhanced_prompt = f"""{prompt}

åŸºäºä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ç”Ÿæˆåˆé€‚çš„Replaceæ›¿æ¢æ¨¡æ¿ï¼š
{files}

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†ææ–‡ä»¶åˆ—è¡¨ä¸­çš„æ–‡ä»¶åæ ¼å¼å’Œç»“æ„
2. æ ¹æ®ç”¨æˆ·è¦æ±‚ç”Ÿæˆèƒ½æ­£ç¡®æå–æ–‡ä»¶ä¿¡æ¯çš„Patternï¼ˆå¦‚æœéœ€è¦ï¼‰
3. ç”Ÿæˆç›¸åº”çš„Replaceæ›¿æ¢æ¨¡æ¿ï¼Œç¡®ä¿ä¸PatternåŒ¹é…æ ¼å¼å…¼å®¹

Replaceç”Ÿæˆè§„åˆ™ï¼š
- å¦‚æœPatternæå–äº†å­£æ•°(S)å’Œé›†æ•°(E)ï¼ŒReplaceåº”è¯¥ä¸º `S{{SXX}}E{{E}}.{{EXT}}`
- å¦‚æœPatternåªæå–äº†é›†æ•°(E)ï¼ŒReplaceåº”è¯¥ä¸º `S01E{{E}}.{{EXT}}` (é»˜è®¤ç¬¬ä¸€å­£)
- å¦‚æœPatternæ²¡æœ‰æå–å­£é›†ä¿¡æ¯ï¼ŒReplaceåº”è¯¥ä¸º `{{ORIGINAL_NAME}}.{{EXT}}`
- å¯¹äºç”µå½±ï¼ŒReplaceé€šå¸¸ä¸º `{{TITLE}}.{{EXT}}`

å¸¸ç”¨å˜é‡ï¼š
- `{{SXX}}`: å­£æ•°ï¼Œæ ¼å¼åŒ–ä¸ºä¸¤ä½æ•°
- `{{E}}`: é›†æ•°ï¼Œä¿æŒåŸå§‹æ ¼å¼
- `{{EXT}}`: æ–‡ä»¶æ‰©å±•å
- `{{TITLE}}`: ç”µå½±/å‰§é›†æ ‡é¢˜
- `{{ORIGINAL_NAME}}`: åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰

å¿…é¡»æ‰§è¡Œçš„æ³¨æ„äº‹é¡¹ï¼š
1. Replaceæ¨¡æ¿å¿…é¡»ä¸Patternçš„æå–ç»„å…¼å®¹
2. ä½¿ç”¨ä¼ ç»Ÿçš„åå‘å¼•ç”¨è¯­æ³•ï¼š\1, \2 ç­‰ï¼Œè€Œä¸æ˜¯${{1}}æˆ–{{1}}
3. Patternä¸­ä¸è¦ä½¿ç”¨å‘½ååˆ†ç»„
4. åªè¿”å›JSONæ ¼å¼ç»“æœï¼š{{"pattern": "...", "replace": "..."}}"""
        ai_analysis = await openapi_chat(
            role="ä½ æ˜¯ä¸€ä¸ªç¼–å†™æ­£åˆ™è¡¨è¾¾å¼çš„ä¸“å®¶ï¼Œå–„äºä»å…ƒç´ åˆ—è¡¨ä¸­é€šè¿‡ç¼–å†™æ­£åˆ™æå–åˆ°æƒ³è¦çš„å…ƒç´ ",
            prompt=enhanced_prompt,
            session=session,
            user_id=user_id
        )

        # æ¸…ç†å¯èƒ½çš„éJSONå†…å®¹
        ai_analysis = ai_analysis.strip()
        if ai_analysis.startswith("```json"):
            ai_analysis = ai_analysis[7:]
        if ai_analysis.endswith("```"):
            ai_analysis = ai_analysis[:-3]

        generate_params = json.loads(ai_analysis)

        generate_params['replace'] = generate_params['replace'].replace("$", "\\")

        return generate_params

    async def ai_generate_params(self, url: str, session, user_id, prompt) -> dict:
        quark_id, stoken, pdir_fid = await self.get_quark_id_stoken_pdir_fid(url=url)
        dir_details = await self.get_quark_dir_detail(quark_id, stoken, pdir_fid, include_dir=False)
        files = [
            {
               "file_name": dir_detail['file_name'],
               "video_max_resolution": dir_detail['video_max_resolution'],
            }
            for dir_detail in dir_details
        ]
        prompt = rf"""\
ç”¨æˆ·è¦æ±‚ï¼š\
{prompt}\

ä½ éœ€è¦åšï¼š
1. è¯·ä»ä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ä¸­æ ¹æ®æ–‡ä»¶çš„file_nameç¼–å†™æ­£åˆ™è¡¨è¾¾å¼åªæå–ç¬¦åˆ<ç”¨æˆ·è¦æ±‚>çš„æ–‡ä»¶å¹¶ä¸”è¿˜éœ€è¦æå–ç¬¦åˆ<ç”¨æˆ·è¦æ±‚>çš„æ–‡ä»¶çš„ file_name ä¸­çš„å­£æ•°å’Œé›†æ•°å’Œæ–‡ä»¶åç¼€ï¼Œå¦‚æœfile_nameä¸­æ²¡æœ‰å­£æ•°çš„ä¿¡æ¯ï¼Œåˆ™åªéœ€è¦æå–é›†æ•°
{files}
2. æ ¹æ®ç¼–å†™çš„æ­£åˆ™åŒ¹é…å¼åŒ¹é…åˆ°file_nameå¹¶æå–åˆ° file_name ä¸­çš„å­£æ•°å’Œé›†æ•°å’Œæ–‡ä»¶åç¼€åï¼Œç”¨äºç”Ÿæˆæ–° file_nameï¼Œå¦‚æœå‰é¢çš„æ­£åˆ™æ²¡æœ‰æå–åˆ°å­£æ•°çš„ä¿¡æ¯ï¼Œåˆ™é»˜è®¤ä¸ºç¬¬ä¸€å­£

ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼Œè¯·ä½ æŒ‰ç…§ç¤ºä¾‹çš„è§„åˆ™è¿›è¡Œç”Ÿæˆï¼š
| pattern                                   | replace                    | æ•ˆæœ                                                                 |
|-------------------------------------------|-----------------------------|----------------------------------------------------------------------|
| `.*`                                      |                             | æ— è„‘è½¬å­˜æ‰€æœ‰æ–‡ä»¶ï¼Œä¸æ•´ç†                                              |
| `\.mp4$`                                  |                             | è½¬å­˜æ‰€æœ‰ `.mp4` åç¼€çš„æ–‡ä»¶                                           |
| `^ã€ç”µå½±TTã€‘èŠ±å¥½æœˆåœ†(\d+)\.(mp4|mkv)`     | `\1.\2`                     | `ã€ç”µå½±TTã€‘èŠ±å¥½æœˆåœ†01.mp4 â†’ 01.mp4`<br>`ã€ç”µå½±TTã€‘èŠ±å¥½æœˆåœ†02.mkv â†’ 02.mkv` |
| `^(\d+)\.mp4`                             | `S02E\1.mp4`                | `01.mp4 â†’ S02E01.mp4`<br>`02.mp4 â†’ S02E02.mp4`                        |
| `^(\d+)\.mp4`                             | `TASKNAME.S02E\1.mp4`     | `01.mp4 â†’ ä»»åŠ¡å.S02E01.mp4`                                         |
| `^S(\d+).*EP(\d+).*.mp4`                             | `S\1E\2.mp4`     | `S02saxEP01.mp4 â†’ ä»»åŠ¡å.S02E01.mp4`                                         |

å¿…é¡»æ‰§è¡Œçš„æ³¨æ„äº‹é¡¹ï¼š
1. å¿…é¡»ä»¥ç¤ºä¾‹çš„è§„åˆ™ç”Ÿæˆ pattern å’Œ replace
2. åœ¨æ­£åˆ™æ›¿æ¢ï¼ˆreplaceï¼‰ä¸­ï¼Œè¯·ä½¿ç”¨ ä¼ ç»Ÿçš„ Python / sed / rename å·¥å…·é£æ ¼ï¼Œä¾‹å¦‚ \1ã€\\1 æˆ– $1ï¼Œ**ä¸è¦ä½¿ç”¨ ${{1:-01}}ã€{{group}}ã€{{1}} ç­‰è¯­æ³•ã€‚
3. patternä¸­ä¸è¦ä½¿ç”¨åˆ†ç»„å‘½åï¼Œå¦‚(?<episode>\d+)\
4. å›ç­”ä¸­åªéœ€è¿”å›JSONæ ¼å¼çš„ç»“æœæ— éœ€è¿”å›å…¶ä»–è¯´æ˜ä¿¡æ¯ï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
    1. pattern: èƒ½å¤ŸåŒ¹é…åˆ°video_max_resolutionä¸º4kçš„æ–‡ä»¶çš„file_nameå¹¶ä¸”è¿˜èƒ½å¤Ÿæå– file_name ä¸­çš„å­£æ•°å’Œé›†æ•°çš„æ­£åˆ™åŒ¹é…å¼
    2. replace: ç”¨äºç”Ÿæˆæ–° file_nameï¼Œå¦‚æœå‰é¢çš„æ­£åˆ™æ²¡æœ‰æå–åˆ°å­£æ•°çš„ä¿¡æ¯ï¼Œåˆ™é»˜è®¤ä¸ºç¬¬ä¸€å­£
"""
        ai_analysis = await openapi_chat(
            role="ä½ æ˜¯ä¸€ä¸ªç¼–å†™æ­£åˆ™è¡¨è¾¾å¼çš„ä¸“å®¶ï¼Œå–„äºä»å…ƒç´ åˆ—è¡¨ä¸­é€šè¿‡ç¼–å†™æ­£åˆ™æå–åˆ°æƒ³è¦çš„å…ƒç´ ",
            prompt=prompt,
            session=session,
            user_id=user_id
        )

        # æ¸…ç†å¯èƒ½çš„éJSONå†…å®¹
        ai_analysis = ai_analysis.strip()
        if ai_analysis.startswith("```json"):
            ai_analysis = ai_analysis[7:]
        if ai_analysis.endswith("```"):
            ai_analysis = ai_analysis[:-3]

        generate_params = json.loads(ai_analysis)

        generate_params['replace'] = generate_params['replace'].replace("$", "\\")

        return generate_params

    async def run_script_now(self, host, task_list):
        resp = requests.post(
            f'{host}/run_script_now?token={self.api_token}',
            json={
                    "tasklist": task_list
                }
        )
        if not resp.ok:
            logger.error(f'Failed to run script {host}, error: {resp.reason}')
        else:
            return resp.text

    async def ai_classify_seasons(self, url: str, session, user_id) -> Tuple[dict, dict]:
        quark_id, stoken, pdir_fid = await self.get_quark_id_stoken_pdir_fid(url=url)
        dir_details = await self.get_quark_dir_detail(quark_id, stoken, pdir_fid, include_dir=True)
        dirname_fid_map = dict()
        for dir_detail in dir_details:
            if dir_detail['dir'] is True:
                dirname_fid_map[dir_detail['file_name']] = dir_detail['fid']

        logger.info(f"æ•´ç†å‰æ–‡ä»¶å¤¹ fidï¼š{dirname_fid_map}")

        prompt = f"""
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­å­—å…¸çš„keyä¸ºæ–‡ä»¶å¤¹åç§°ï¼Œç°åœ¨éœ€è¦ä½ å¸®æˆ‘æå–å‡ºå…¶ä¸­æ–‡ä»¶å¤¹åç§°ä¸ç”µè§†å‰§å­£æ•°æœ‰å…³çš„æ•°æ®:
{dirname_fid_map}
è¦æ±‚ï¼š
1. æ–‡ä»¶å¤¹åç§°ä¸ç”µè§†å‰§å­£æ•°æ— å…³çš„éœ€è¦æ’é™¤
2. åŒ¹é…åˆ°ä¸ç”µè§†å‰§å­£æ•°ç›¸å…³çš„æ–‡ä»¶å¤¹åç§°åéœ€è¦é‡å‘½åä¸ºã€ŒSeason {{å­£æ•°}}ã€ï¼Œå…¶ä¸­{{å­£æ•°}}ä¸ºä¸¤ä¸ªæ•°å­—ï¼Œä¾‹å¦‚å¦‚ç¬¬ä¸€å­£ä¸ºSeason 01ï¼Œç¬¬ä¸‰å­£ä¸ºSeason 03ï¼Œç¬¬åä¸€å­£ä¸ºSeason 11
3. è¾“å‡ºä¸€ä¸ªå­—å…¸ï¼Œkeyä¸ºé‡å‘½åå‰çš„æ–‡ä»¶å¤¹åç§°ï¼Œvalueä¸ºé‡å‘½ååçš„ç”µè§†å‰§å­£æ•°

ä¾‹å­ï¼š
1. 
è¾“å…¥ï¼š
{{
    'çº¸é’å±‹S01': 'e6b8a848e57c4988b719facff5a1b45f',
    'çº¸é’å±‹S02': '569e17e641fc46638c2b9ea06f278d12',
    'çº¸é’å±‹S03': 'b696a6df6bfb447f9c638858bdbcd3e4',
    'çº¸é’å±‹S04': '94f8753593f647f1af26089230d79e76',
    'çº¸é’å±‹S05': 'f0529a63ac464e299e39b0caa3001df3',
    'ç›¸å…³å›¾ç‰‡': "f0529a63ac464e299e39bdddd001df3"
}}
è¾“å‡ºï¼š
{{
    'çº¸é’å±‹S01': 'Season 01',
    'çº¸é’å±‹S02': 'Season 02',
    'çº¸é’å±‹S03': 'Season 03',
    'çº¸é’å±‹S04': 'Season 04',
    'çº¸é’å±‹S05': 'Season 05'
}}

2. 
è¾“å…¥ï¼š
{{
    'é¾™æ—ç¬¬äºŒå­£': 'e6b8a848e57c4988b719facff5a1b45f',
    'é¾™æ—ç¬¬äº”å­£': '569e17e641fc46638c2b9ea06f278d12',
    'æ›´æ–°æé†’': "f0529a63ac464e299e39bdddd001df3"
}}
 
è¾“å‡ºï¼š
{{
    'é¾™æ—ç¬¬äºŒå­£': 'Season 02',
    'é¾™æ—ç¬¬äº”å­£': 'Season 05',
}}
 
"""
        ai_analysis = await openapi_chat(
            role="ä½ æ˜¯ä¸€ä¸ªå½±è§†å‰§å­£æ•°åˆ†ç±»ä¸“å®¶ï¼Œå–„äºä»å¤šä¸ªæ–‡ä»¶å¤¹åˆ—è¡¨ä¸­æ‰¾åˆ°å’Œå½±è§†å‰§å­£æ•°ç›¸å…³çš„æ–‡ä»¶å¤¹",
            prompt=prompt,
            session=session,
            user_id=user_id
        )

        # æ¸…ç†å¯èƒ½çš„éJSONå†…å®¹
        ai_analysis = ai_analysis.strip()
        if ai_analysis.startswith("```json"):
            ai_analysis = ai_analysis[7:]
        if ai_analysis.endswith("```"):
            ai_analysis = ai_analysis[:-3]

        extract_seasons = json.loads(ai_analysis)
        extract_seasons = dict(sorted(extract_seasons.items(), key=lambda x: int(x[1].split()[1])))
        logger.info(f"aiè¯†åˆ«æ–‡ä»¶å¤¹å­£æ•°ç»“æœï¼š{extract_seasons}")

        seasons_fid = dict()

        for dirname, fid in dirname_fid_map.items():
            if dirname in extract_seasons:
                seasons_fid.update({
                    extract_seasons[dirname]: fid
                })

        logger.info(f"æ•´ç†åç»“æœ {seasons_fid}")
        return seasons_fid, extract_seasons



if __name__ == '__main__':
    qas = QuarkAutoDownload(api_token='')
    url = "https://pan.quark.cn/s/351409f4f293#/list/share/e9f31ac4f4fc4832a01315787d834613"
    asyncio.run(qas.ai_classify_seasons(url))